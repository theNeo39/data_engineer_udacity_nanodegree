# **Data Modeling with Postgres**

## **Project Overview**
A startup called Sparkify wants to analyze the data they've been collecting on songs and user activity on their new music streaming app. All the data is in JSON format and they would like to have a Postgres database with star schema data modelling to optimize the queries for analysis and bulid ETL pipeline using python.They are particularly interested in understanding what songs users are listening to.

## **Song Dataset**
This dataset is a subset of [Million Song Dataset](http://millionsongdataset.com/)  
A single song json file looks like:  
```
{"num_songs": 1, "artist_id": "ARJIE2Y1187B994AB7", "artist_latitude": null, "artist_longitude": null, "artist_location": "","artist_name": "Line Renaud", "song_id": "SOUPIRU12A6D4FA1E1", "title": "Der Kleine Dompfaff", "duration": 152.92036, "year": 0}
``` 

## **log Dataset**
This dataset is generated by [Event Simulator](https://github.com/Interana/eventsim)  
A single events json file looks like:  
```
{"artist": null, "auth": "Logged In", "firstName": "Walter", "gender": "M", "itemInSession": 0, "lastName": "Frye", "length": null,"level": "free", "location": "SanFrancisco-Oakland-Hayward, CA", "method": "GET","page": "Home", "registration": 15409191667960,"sessionId": 38, "song": null, "status": 200, "ts": 1541105830796,"userAgent": "\"Mozilla\/5.0 (Macintosh; Intel Mac OS X 10_9_4AppleWebKit\/537.36 (KHTML, like Gecko) Chrome\/36.0.1985.143 Safari\/537.36\"", "userId": "39"}
```


## **Database Schema**
Here we have considered **star schema** for data modeling because we are focused on analysing the data ie.. more read operations. 

#### **Fact Table**

**songplays** -records the log data associated with song plays with page NextSong.

```
songplay_id, start_time, user_id, level, song_id, artist_id, session_id, location, user_agent
```

#### **Dimesion Tables**

**users** -users in the app

```
user_id, first_name, last_name, gender, level
```

**songs** -songs in music database

```
song_id, title, artist_id, year, duration
```

**artists** -artists in music database

```
artist_id, name, location, latitude, longitude
```

**time** - timestamps of records in songplays broken down into specific units

```
start_time, hour, day, week, month, year, weekday
```

## **ETL Pipeline**

1. Extracted the necessary columns from the JSON files.
2. Performing the necessary transformation to get desired information.
3. Loading the data into fact and dimension tables.

## **Project Files**

* **create_tables.py** - it contains code for setting up the database *sparkifydb* and tables.

* **sql_queries.py** - this contains SQL queries for *creation*, *dropping*, *inserting* table.

* **etl.py** - this has the code to perform necessary operations to load data into tables.

## **How To Run**

Run the following code in sequence to get perform the ETL 

`python create_tables.py`  
`python etl.py`

## **References**

* [PostgreSQL Tutorial](https://www.postgresqltutorial.com/)
* [pyscopg Doc](https://www.psycopg.org/docs/)
* [postgreSQL Doc](https://www.postgresql.org/)

